{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This script loads an existing QuPath project,\n",
    "reads all images with their annotations and/or detections\n",
    "and classifies them using a Deep Learning model of your choice.\n",
    "It outputs a new QuPath project with the same images and\n",
    "classified annotations.\n",
    "If an image does not contain annotations, the image will be skipped\n",
    "and will not be added to the new QuPath project.\n",
    "\n",
    "This code is based on code from Andrew J.\n",
    "(https://github.com/choosehappy/QuPathGeoJSONImportExport)\n",
    "but makes use of the paquo library (https://paquo.readthedocs.io)\n",
    "\n",
    "Written by Sabina K. and Andrew J."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import shutil\n",
    "import tiffslide as openslide\n",
    "from tqdm.notebook import tqdm\n",
    "from math import ceil\n",
    "import matplotlib.pyplot as plt\n",
    "from shapely.geometry import shape\n",
    "from shapely.strtree import STRtree\n",
    "from shapely.geometry import Polygon\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "import cv2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from paquo.projects import QuPathProject\n",
    "from paquo.images import QuPathImageType\n",
    "from paquo.classes import QuPathPathClass\n",
    "from paquo.colors import QuPathColor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": [
     "parameters"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_NAME = \"project1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PROJECT_FILE = f\"{PROJECT_NAME}/project.qpproj\"\n",
    "PROJECT_PATH = f\"{os.getcwd()}/{PROJECT_FILE}\"\n",
    "NEW_PROJECT_NAME = f\"{PROJECT_NAME}_out\"\n",
    "NEW_PROJECT_FILE = f\"{PROJECT_NAME}_out/project.qpproj\"\n",
    "NEW_PROJECT_PATH = f\"{os.getcwd()}/{NEW_PROJECT_FILE}\"\n",
    "\n",
    "DEVICE = torch.device('cuda')\n",
    "# DEVICE = torch.device('cpu')\n",
    "OPENSLIDELEVEL = 0  # Level from openslide to read\n",
    "TILESIZE = 10000  # Size of the tile to load from openslide\n",
    "PATCHSIZE = 32  # Patch size needed by our DL model\n",
    "MINHITS = 100  # The minimum number of objects needed to be present within a tile for the tile to be computed on\n",
    "BATCHSIZE = 1024  # How many patches we want to send to the GPU at a single time\n",
    "NUM_OF_CLASSES = 2  # Number of output classes our model is providing\n",
    "CLASSNAMES = [\"Other\", \"Lymphocyte\"]  # The names of those classes which will appear in QuPath later on\n",
    "CLASSCOLORS = [-377282, -9408287]  # Their associated color, see selection of different color values at the bottom of the file\n",
    "MASK_PATCHES = False  # If we would like to black out the space around the object of interest, this is determined by how the model was trained\n",
    "# MODEL_FNAME=\"lymph_model.pth\"  # DL model to use\n",
    "\n",
    "\n",
    "# ---- Load your model here\n",
    "# model = LoadYourModelHere().to(DEVICE)\n",
    "# checkpoint = torch.load(MODEL_FNAME, map_location=lambda storage, loc: storage)  # load checkpoint to CPU and then put to device https://discuss.pytorch.org/t/saving-and-loading-torch-models-on-2-machines-with-different-number-of-gpu-devices/6666\n",
    "# model.load_state_dict(checkpoint[\"model_dict\"])\n",
    "# model.eval()\n",
    "# summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_qupath_annotations(image):\n",
    "    \"\"\"Read annotations from QuPath image, return list\"\"\"\n",
    "    annotations = image.hierarchy.annotations  # annotations are accessible via the hierarchy\n",
    "    print(f\"Image {image.image_name} has {len(annotations)} annotations.\")\n",
    "    ann = [annotation.roi for annotation in annotations] if annotations else list()  # Return empty list if annotations are missing\n",
    "    return ann\n",
    "\n",
    "\n",
    "def read_qupath_detections(image):\n",
    "    \"\"\"Read detections from QuPath image, return list\"\"\"\n",
    "    detections = image.hierarchy.detections  # detections are stored in a set like proxy object\n",
    "    print(f\"Image {image.image_name} has {len(detections)} detections.\")  # detections don't have a path_class.name\n",
    "    det = [detection.roi for detection in detections] if detections else list()\n",
    "    return det\n",
    "\n",
    "\n",
    "def add_qupath_classes(classnames: list, colors: list, qp):\n",
    "    \"\"\"Add custom classes and corresponding colors to QuPath project\"\"\"\n",
    "    new_classes = []\n",
    "    for class_name, class_color in zip(classnames, colors):\n",
    "        new_classes.append(\n",
    "            QuPathPathClass(name=class_name,\n",
    "                            color=QuPathColor.from_java_rgba(class_color))\n",
    "        )\n",
    "    qp.path_classes = new_classes  # Setting QuPathProject.path_class always replaces all classes\n",
    "    print(\"Adding project classes to new QuPath project:\")\n",
    "    for path_class in qp.path_classes:\n",
    "        print(f\"'{path_class.name}'\")\n",
    "\n",
    "\n",
    "def find_tile(tilesize: int, searchtree: STRtree, scalefactor: int, y, x):\n",
    "    \"\"\"Create a search polygon and find matches in the searchtree (STRtree)\"\"\"\n",
    "    searchtile = Polygon([[x, y],\n",
    "                          [x + tilesize * scalefactor, y],\n",
    "                          [x + tilesize * scalefactor, y + tilesize * scalefactor],\n",
    "                          [x, y + tilesize * scalefactor]])\n",
    "    hits = searchtree.query(searchtile)\n",
    "    return hits, searchtile\n",
    "\n",
    "\n",
    "def get_tile(openslidelevel: int, tilesize: int, osh, paddingsize: int, y, x):\n",
    "    \"\"\"Load an image tile and put the RGB values in a np.array\"\"\"\n",
    "    # Using tiffslide we can directly load as_array.\n",
    "    tile = osh.read_region((x - paddingsize, y - paddingsize), openslidelevel,\n",
    "                           (tilesize + 2 * paddingsize, tilesize + 2 * paddingsize),\n",
    "                           as_array=True)[:, :, 0:3]  # Trim alpha\n",
    "    return tile\n",
    "\n",
    "\n",
    "def construct_mask(allshapes: list, scalefactor: int, paddingsize: int, int_coords, y, x, hits: list, tile):\n",
    "    mask = np.zeros((tile.shape[0:2]), dtype=tile.dtype)\n",
    "    exteriors = [int_coords(allshapes[hit.id].boundary.coords) for hit in hits]\n",
    "    exteriors_shifted = [(ext - np.asarray([(x - paddingsize), (y - paddingsize)]))\n",
    "                         // scalefactor for ext in exteriors]\n",
    "    cv2.fillPoly(mask, exteriors_shifted, 1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_maskpatch(patchsize, mask, c, r, patch):\n",
    "    maskpatch = mask[r - patchsize // 2:r + patchsize // 2,\n",
    "                c - patchsize // 2:c + patchsize // 2]\n",
    "    patch = np.multiply(patch, maskpatch[:, :, None])\n",
    "    return patch\n",
    "\n",
    "\n",
    "def divide_batch(arr, size: int):\n",
    "    for i in range(0, arr.shape[0], size):\n",
    "        yield arr[i:i + size, ::]\n",
    "\n",
    "\n",
    "def process_batch(arr_out, hits: list):\n",
    "    classids = []\n",
    "    for batch_arr in tqdm(divide_batch(arr_out, BATCHSIZE), leave=False):\n",
    "        # batch_arr_gpu = torch.from_numpy(batch_arr.transpose(\n",
    "        #     0, 3, 1, 2)).type('torch.FloatTensor').to(DEVICE) / 255\n",
    "        # Get results\n",
    "        # classids.append(torch.argmax(model.img2class(batch_arr_gpu), dim=1).detach().cpu().numpy())\n",
    "        classids.append(np.random.choice([0, 1], batch_arr.shape[0]))\n",
    "\n",
    "    classids = np.hstack(classids)\n",
    "\n",
    "    for hit, classid in zip(hits, classids):\n",
    "        hit.class_id = classid\n",
    "\n",
    "\n",
    "def add_annotations(qpout, entry, ann: list, allshapes: list):\n",
    "    for classified_shape in allshapes:\n",
    "        annotation = entry.hierarchy.add_annotation(roi=classified_shape,\n",
    "                                                    path_class=qpout.path_classes[classified_shape.class_id]\n",
    "                                                    if hasattr(classified_shape, \"class_id\")\n",
    "                                                    else None)\n",
    "        annotation.name = str(classified_shape.geom_type)  # We can also add a name to the annotations\n",
    "\n",
    "    if ann:  # Add the annotations to the new project as they were\n",
    "        for annotation_shape in ann:\n",
    "            entry.hierarchy.add_annotation(roi=annotation_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "def project_cleanup():\n",
    "    qpout = QuPathProject(NEW_PROJECT_PATH)\n",
    "    for image in qpout.images:\n",
    "        if not image.hierarchy.annotations:\n",
    "            print(f\"Removing new QuPath project '{qpout.name}'. No need to keep project without annotations.\")\n",
    "            shutil.rmtree(NEW_PROJECT_NAME)\n",
    "        else:\n",
    "            print(f\"Done. Please look at QuPath project '{qpout.name}' in QuPath.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    with QuPathProject(NEW_PROJECT_PATH, mode='a') as qpout:\n",
    "        print(f\"Created new QuPath project: '{qpout.name}'.\")\n",
    "        add_qupath_classes(CLASSNAMES, CLASSCOLORS, qpout)\n",
    "\n",
    "        qp = QuPathProject(PROJECT_PATH, mode='r')\n",
    "        print(f\"Opened project ‘{qp.name}’ \")\n",
    "        print(f\"Project has {len(qp.images)} image(s).\")\n",
    "        try:\n",
    "            image = qp.images[0]\n",
    "\n",
    "            ann = read_qupath_annotations(image)  # We keep the annotations, but we don't classify them\n",
    "            det = read_qupath_detections(image)\n",
    "\n",
    "            if det:\n",
    "                allshapes = det  # We only want to classify the detections\n",
    "\n",
    "                searchtree = STRtree(allshapes)\n",
    "                wsi_fname = image.uri.split(\":\")[1]\n",
    "                entry = qpout.add_image(wsi_fname, image_type=QuPathImageType.BRIGHTFIELD_H_E,\n",
    "                                        allow_duplicates=True)\n",
    "                osh = openslide.OpenSlide(wsi_fname)\n",
    "                nrow, ncol = osh.level_dimensions[0]\n",
    "                nrow = ceil(nrow / TILESIZE)\n",
    "                ncol = ceil(ncol / TILESIZE)\n",
    "                scalefactor = int(osh.level_downsamples[OPENSLIDELEVEL])\n",
    "                paddingsize = PATCHSIZE // 2 * scalefactor\n",
    "\n",
    "                int_coords = lambda x: np.array(x).round().astype(np.int32)\n",
    "\n",
    "                # Now lets start finding interesting tiles to operate on\n",
    "                for y in tqdm(range(0, osh.level_dimensions[0][1], round(TILESIZE * scalefactor)), desc=\"outer\",\n",
    "                              leave=False):\n",
    "                    for x in tqdm(range(0, osh.level_dimensions[0][0], round(TILESIZE * scalefactor)),\n",
    "                                  desc=f\"inner {y}\", leave=False):\n",
    "                        hits, searchtile = find_tile(TILESIZE, searchtree, scalefactor, y, x)\n",
    "                        hits = [hit for hit in hits if hit.centroid.intersects(searchtile)]  # filter by centroid\n",
    "\n",
    "                        if len(hits) < MINHITS:\n",
    "                            continue\n",
    "\n",
    "                        tile = get_tile(OPENSLIDELEVEL, TILESIZE, osh, paddingsize, y, x)\n",
    "\n",
    "                        if MASK_PATCHES:\n",
    "                            mask = construct_mask(allshapes, scalefactor, paddingsize,\n",
    "                                                  int_coords, y, x, hits, tile)\n",
    "\n",
    "                        arr_out = np.zeros((len(hits), PATCHSIZE, PATCHSIZE, 3))\n",
    "                        # Get patches from hits within this tile and stick them (and their ids) into matrices\n",
    "                        for hit, arr in zip(hits, arr_out):\n",
    "                            px, py = hit.centroid.coords[:][0]  # Faster than hit.x and hit.y, likely because of call stack overhead\n",
    "                            c = int((px - x + paddingsize) // scalefactor)\n",
    "                            r = int((py - y + paddingsize) // scalefactor)\n",
    "                            patch = tile[r - PATCHSIZE // 2:r + PATCHSIZE // 2,\n",
    "                                         c - PATCHSIZE // 2:c + PATCHSIZE // 2, :]\n",
    "\n",
    "                            if MASK_PATCHES:\n",
    "                                patch = get_maskpatch(PATCHSIZE, mask, c, r, patch)\n",
    "\n",
    "                            arr[:] = patch\n",
    "\n",
    "                        # Process batch\n",
    "                        process_batch(arr_out, hits)\n",
    "\n",
    "                add_annotations(qpout, entry, ann, allshapes)\n",
    "\n",
    "            else:\n",
    "                print(\"No detections in this image.\")\n",
    "\n",
    "            print(f\"Done. Please look at QuPath project '{qpout.name}' in QuPath.\")\n",
    "            # project_cleanup()\n",
    "\n",
    "        except IndexError:\n",
    "            print(\"No images to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "#         \"name\": \"Positive\",  # add colors\n",
    "#         \"colorRGB\": -377282\n",
    "#         \"name\": \"Other\",\n",
    "#         \"colorRGB\": -14336\n",
    "#         \"name\": \"Stroma\",\n",
    "#         \"colorRGB\": -6895466\n",
    "#         \"name\": \"Necrosis\",\n",
    "#         \"colorRGB\": -13487566\n",
    "#         \"name\": \"Tumor\",\n",
    "#         \"colorRGB\": -3670016\n",
    "#         \"name\": \"Immune cells\",\n",
    "#         \"colorRGB\": -6268256\n",
    "#         \"name\": \"Negative\",\n",
    "#         \"colorRGB\": -9408287\n",
    "\n",
    "# This code to perform entire polygon with complex objects\n",
    "# exteriors = [int_coords(geo.coords) for hit in hits for geo in hit.boundary.geoms]  # Need this modification for complex structures\n",
    "# This code to perform by center with complex objects\n",
    "# exteriors = [int_coords(geo.coords) for hit in hits for geo in allshapes[hit.id].boundary.geoms]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}